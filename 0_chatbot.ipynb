{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative-ai/blob/main/0_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfuohDidliSh"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfDWxVPylmyT"
      },
      "source": [
        "## 1. Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiUrIGPdl7Ns"
      },
      "source": [
        "Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EPC2bgSPlyCp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq -U faiss-cpu\n",
        "#!pip install -qqq -U langchain\n",
        "pip install -qqq -U langchain-community\n",
        "pip install -qqq -U langchain-huggingface\n",
        "pip install -qqq -U pypdf\n",
        "pip install -qqq -U streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQgxaRFGKuhZ",
        "outputId": "b0c551a7-b8d2-4514-b84c-2a4f80ec9a35"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq -U jupyter\n",
        "pip install -qqq -U ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAj83CxIl_or"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yh_NskAsmA-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zklDMk5l2KR"
      },
      "source": [
        "Colab: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEViDb6ql5zq"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIaJTonJmUQ2"
      },
      "source": [
        "Local: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBvVTgaAmV3w"
      },
      "outputs": [],
      "source": [
        "# token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpffJlkmcgW"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uWefNZbqmdsh"
      },
      "outputs": [],
      "source": [
        "hf_model = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "# hf_model = 'microsoft/Phi-3.5-mini-instruct'\n",
        "\n",
        "# hf_model = 'Qwen/Qwen2.5-7B-Instruct' # empty outputs\n",
        "\n",
        "# hf_model = 'mistralai/Mistral-Nemo-Instruct-2407' # long time no response\n",
        "# hf_model = 'microsoft/phi-4-gguf' # ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\")\n",
        "# hf_model = 'microsoft/phi-4' # The model microsoft/phi-4 is too large to be loaded automatically (29GB > 10GB).\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OndazxOCxVn9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b84e6c0160c6414abfd329065b84d189",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Gastan\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gastan\\Documents\\Data Science\\generative-ai\\data\\cache\\models--sentence-transformers--all-MiniLM-l6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0286b4c946a24f3681255d79bed1236b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c10ecfbbe754c74a1d9bbfa72e5a116",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e26fa26937a49429d012d3df7dd58d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b8be79e800e47d2819085205daf2838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccc7bbff2e62431da9ef6f7d1c01f062",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20af12bae9144127bcb177b39a5e110f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaea436aad94438f9d837487c9d8e394",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aadf89852f1546b49346063331ede8dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8a1b1472d324552b19efc03522b0a2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aabfb974192e4f13b99f47862d2fde46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# embedding_model = 'Qwen/Qwen2.5-7B-Instruct'\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-l6-v2'\n",
        "embeddings_folder = \"data/cache/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGg-llCPoe9i"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rumFfy1FxBkI"
      },
      "source": [
        "Create vector from loaded document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GHd5BrWEKuhp"
      },
      "outputs": [],
      "source": [
        "# # The document\n",
        "# # file_id = '19dUK3V5K8cvu1E5uPzdx_G6uPXMT1zFw' # Google drive ID: CIA world factbook 2018-2019\n",
        "# # file_id = '1uROXFibYrryFmJmd-oD3edD4o4bg_9iu' # Google drive ID: CIA world factbook 2021-2022\n",
        "# # file_id = '19gAxv0fhFpu_dLrd0jlNNCbOOIAMTtpt' # Google drive ID: CIA world factbook 2023-2024\n",
        "# # file = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
        "# file = 'data/The CIA World Factbook 2023-2024.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aVfsmlY4Kuhp"
      },
      "outputs": [],
      "source": [
        "# # Read pages\n",
        "# loader = PyPDFLoader(file)\n",
        "# pages = []\n",
        "# async for page in loader.alazy_load():\n",
        "#     pages.append(page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1h6_PpaHw8aE"
      },
      "outputs": [],
      "source": [
        "# # Split text\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
        "# docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VliUjeE2Kuhq"
      },
      "outputs": [],
      "source": [
        "# # Review the loaded document\n",
        "# print(f\"{docs[10].metadata}\\n\")\n",
        "# print(docs[10].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oYSGzYvRyGNb"
      },
      "outputs": [],
      "source": [
        "# # Create vector\n",
        "# vector_db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uNcrjSojKuhr"
      },
      "outputs": [],
      "source": [
        "# # Save vector\n",
        "# vector_db.save_local(\"data/CIA_2023_2024_faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXRITtPxFpy"
      },
      "source": [
        "Load vector from saved index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iiHxzE_AohNs"
      },
      "outputs": [],
      "source": [
        "vector_db = FAISS.load_local(\"data/CIA_2023_2024_faiss_index\", embeddings, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSS-OU-IycZh"
      },
      "source": [
        "Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3Q__zz6VybMq"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZw8ojnZomfR"
      },
      "source": [
        "Chat setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y9Tbv_08on9W"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "Answer the question based only on the following context and previous conversation.\n",
        "Keep your answers very short and succinct. Give me only 1 question-answer, no more.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\n",
        "\"\"\"\n",
        "#, informative, and clear, so that the couterpart can learn from you.\n",
        "# chat_history = []\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "doc_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, prompt\n",
        ")\n",
        "\n",
        "doc_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDclu2qqZh8"
      },
      "source": [
        "Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG89RzK5qcBS",
        "outputId": "161e1049-85df-4914-8ebb-95ef1061bb5b"
      },
      "outputs": [],
      "source": [
        "chain = create_retrieval_chain(\n",
        "    doc_retriever, doc_chain\n",
        ")\n",
        "history = []\n",
        "# Start the conversation loop\n",
        "while True:\n",
        "  user_input = input(\"\\nYou: \")\n",
        "\n",
        "  # Check for exit condition\n",
        "  if user_input.lower() == 'end':\n",
        "      print(\"Ending the conversation. Goodbye!\")\n",
        "      break\n",
        "\n",
        "  # Get the response from the conversation chain\n",
        "  response = chain.invoke({\"input\":user_input, \"chat_history\": history, \"context\": retriever})\n",
        "  history.extend([{\"role\": \"human\", \"content\": response[\"input\"]},{\"role\": \"assistant\", \"content\":response[\"answer\"]}])\n",
        "  # Print the chatbot's response\n",
        "  print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cED7Wah0l8l"
      },
      "source": [
        "Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVq050eE02v9",
        "outputId": "8d218ff7-b28e-4b9d-96cf-01e00b477db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data/CIA_2023_2024_streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/CIA_2023_2024_streamlit_app.py\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "# hf_model = 'microsoft/Phi-3.5-mini-instruct'\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-l6-v2'\n",
        "embeddings_folder = 'data/cache/'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local('data/CIA_2023_2024_faiss_index', embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "Answer the question based only on the following context and previous conversation.\n",
        "Keep your answers short, succinct. Give only one question-answer.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "# , informative, and clear, so that the couterpart can learn from you.\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', template),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human', '{input}'),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title('CIA World Factbook 2023-2024')\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input('Want to know CIA secrets?'):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message('human').markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({'role': 'human', 'content': prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner('Asking CIA...'):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({'input': prompt, 'chat_history': st.session_state.messages, 'context': retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer['answer']\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message('assistant'):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({'role': 'assistant', 'content':  response})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "0_chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
