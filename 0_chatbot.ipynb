{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative_ai/blob/main/0_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfuohDidliSh"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfDWxVPylmyT"
      },
      "source": [
        "## 1. Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiUrIGPdl7Ns"
      },
      "source": [
        "Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EPC2bgSPlyCp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq -U faiss-cpu\n",
        "#!pip install -qqq -U langchain\n",
        "pip install -qqq -U langchain-community\n",
        "pip install -qqq -U langchain-huggingface\n",
        "pip install -qqq -U pypdf\n",
        "pip install -qqq -U streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq -U jupyter\n",
        "pip install -qqq -U ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAj83CxIl_or"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yh_NskAsmA-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zklDMk5l2KR"
      },
      "source": [
        "Colab: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vEViDb6ql5zq"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIaJTonJmUQ2"
      },
      "source": [
        "Local: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OBvVTgaAmV3w"
      },
      "outputs": [],
      "source": [
        "# token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpffJlkmcgW"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uWefNZbqmdsh"
      },
      "outputs": [],
      "source": [
        "# hf_model = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "hf_model = 'microsoft/Phi-3.5-mini-instruct'\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LttoXR4knZ4I"
      },
      "source": [
        "This will not be needed when everything on github:\n",
        "\n",
        "https://drive.google.com/file/d/14PqeK1VNokE5PZ-b-hVLqV-h4xnSR7B4/view?usp=sharing Alice in Wonderland\n",
        "\n",
        "https://drive.google.com/file/d/1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb/view?usp=sharing Statistics in Python\n",
        "\n",
        "https://drive.google.com/file/d/1i5ZhAxtIown7RzH1Sh13y2eqcXKvsMyq/view?usp=sharing Statistics in R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGg-llCPoe9i"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OndazxOCxVn9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Gastan\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gastan\\Documents\\Data Science\\generative-ai\\data\\cache\\models--sentence-transformers--all-MiniLM-l6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"data/cache/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rumFfy1FxBkI"
      },
      "source": [
        "Create vector from loaded document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # The document\n",
        "# # # file_id = '1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb' # Google drive file's ID\n",
        "# # # file = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
        "# file = 'data/The CIA World Factbook 2018-2019.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Read pages\n",
        "# loader = PyPDFLoader(file)\n",
        "# pages = []\n",
        "# async for page in loader.alazy_load():\n",
        "#     pages.append(page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1h6_PpaHw8aE"
      },
      "outputs": [],
      "source": [
        "# # Split text\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
        "# docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Review the loaded document\n",
        "# print(f\"{docs[10].metadata}\\n\")\n",
        "# print(docs[10].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYSGzYvRyGNb"
      },
      "outputs": [],
      "source": [
        "# # Create vector\n",
        "# vector_db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Save vector\n",
        "# vector_db.save_local(\"data/CIA_faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXRITtPxFpy"
      },
      "source": [
        "Load vector from saved index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iiHxzE_AohNs"
      },
      "outputs": [],
      "source": [
        "vector_db = FAISS.load_local(\"data/CIA_faiss_index\", embeddings, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSS-OU-IycZh"
      },
      "source": [
        "Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3Q__zz6VybMq"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZw8ojnZomfR"
      },
      "source": [
        "Chat setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y9Tbv_08on9W"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "Answer the question based only on the following context and previous conversation.\n",
        "Keep your answers short, succinct, informative, and clear, so that the couterpart can learn from you.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# chat_history = []\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "doc_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, prompt\n",
        ")\n",
        "\n",
        "doc_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDclu2qqZh8"
      },
      "source": [
        "Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG89RzK5qcBS",
        "outputId": "1cf6882d-7796-4b6a-f1ff-ac66ce430ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response: Environmental issues and international agreements related to conservation, deforestation, desertification, freshwater resources, climate change, biodiversity, and pollution.\n",
            "Ending the conversation. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "chain = create_retrieval_chain(\n",
        "    doc_retriever, doc_chain\n",
        ")\n",
        "history = []\n",
        "# Start the conversation loop\n",
        "while True:\n",
        "  user_input = input(\"\\nYou: \")\n",
        "\n",
        "  # Check for exit condition\n",
        "  if user_input.lower() == 'end':\n",
        "      print(\"Ending the conversation. Goodbye!\")\n",
        "      break\n",
        "\n",
        "  # Get the response from the conversation chain\n",
        "  response = chain.invoke({\"input\":user_input, \"chat_history\": history, \"context\": retriever})\n",
        "  history.extend([{\"role\": \"human\", \"content\": response[\"input\"]},{\"role\": \"assistant\", \"content\":response[\"answer\"]}])\n",
        "  # Print the chatbot's response\n",
        "  print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cED7Wah0l8l"
      },
      "source": [
        "Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRAfwcEV1IPi"
      },
      "source": [
        "app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFsof3G1NHL",
        "outputId": "b1f38341-704f-4695-da4e-bb7db31cb9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VVq050eE02v9",
        "outputId": "212957fe-6528-4caf-80cf-db8abb210f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data/streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/streamlit_app.py\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = 'microsoft/Phi-3.5-mini-instruct' # 'mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-l6-v2'\n",
        "embeddings_folder = 'data/cache/'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local('data/CIA_faiss_index', embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "Answer the question based only on the following context and previous conversation.\n",
        "Keep your answers short, succinct, informative, and clear, so that the couterpart can learn from you.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', template),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human', '{input}'),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title('CIA World Factbook 2018-2019')\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input('Curious minds wanted!'):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message('human').markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({'role': 'human', 'content': prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner('Asking CIA...'):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({'input': prompt, 'chat_history': st.session_state.messages, 'context': retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer['answer']\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message('assistant'):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({'role': 'assistant', 'content':  response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code necessary for Colab only\n",
        "\n",
        "import os\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "def tunnel_prep():\n",
        "    for f in ('cloudflared-linux-amd64', 'logs.txt', 'nohup.out'):\n",
        "        try:\n",
        "            os.remove(f'data/cache/{f}')\n",
        "            print(f\"Deleted {f}\")\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "    !nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "    url = \"\"\n",
        "    while not url:\n",
        "        time.sleep(1)\n",
        "        result = !grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1\n",
        "        if result:\n",
        "            url = result[0]\n",
        "    return display(HTML(f'Your tunnel URL <a href=\"{url}\" target=\"_blank\">{url}</a>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Background processes not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtunnel_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mtunnel_prep\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchmod +x cloudflared-linux-amd64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[0;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mOSError\u001b[0m: Background processes not supported."
          ]
        }
      ],
      "source": [
        "tunnel_prep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Background processes not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstreamlit run data/rag_app.py &>data/cache/logs.txt &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[0;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mOSError\u001b[0m: Background processes not supported."
          ]
        }
      ],
      "source": [
        "!streamlit run data/rag_app.py &>data/cache/logs.txt &"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "0_chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
