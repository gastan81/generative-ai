{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative_ai/blob/main/0_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfuohDidliSh"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfDWxVPylmyT"
      },
      "source": [
        "## 1. Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiUrIGPdl7Ns"
      },
      "source": [
        "Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EPC2bgSPlyCp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq -U faiss-cpu\n",
        "#!pip install -qqq -U langchain\n",
        "pip install -qqq -U langchain-community\n",
        "pip install -qqq -U langchain-huggingface\n",
        "pip install -qqq -U pypdf\n",
        "pip install -qqq -U streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAj83CxIl_or"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yh_NskAsmA-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zklDMk5l2KR"
      },
      "source": [
        "Colab: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vEViDb6ql5zq"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIaJTonJmUQ2"
      },
      "source": [
        "Local: token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OBvVTgaAmV3w"
      },
      "outputs": [],
      "source": [
        "# token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpffJlkmcgW"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uWefNZbqmdsh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Gastan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# hf_model = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "hf_model = 'microsoft/Phi-3.5-mini-instruct'\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LttoXR4knZ4I"
      },
      "source": [
        "This will not be needed when everything on github:\n",
        "\n",
        "https://drive.google.com/file/d/14PqeK1VNokE5PZ-b-hVLqV-h4xnSR7B4/view?usp=sharing Alice in Wonderland\n",
        "\n",
        "https://drive.google.com/file/d/1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb/view?usp=sharing Statistics in Python\n",
        "\n",
        "https://drive.google.com/file/d/1i5ZhAxtIown7RzH1Sh13y2eqcXKvsMyq/view?usp=sharing Statistics in R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGg-llCPoe9i"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OndazxOCxVn9"
      },
      "outputs": [],
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rumFfy1FxBkI"
      },
      "source": [
        "Create vector from loaded document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # The document\n",
        "# # # file_id = '1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb' # Google drive file's ID\n",
        "# # # file = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
        "# file = 'data/The CIA World Factbook 2018-2019.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Read pages\n",
        "# loader = PyPDFLoader(file)\n",
        "# pages = []\n",
        "# async for page in loader.alazy_load():\n",
        "#     pages.append(page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1h6_PpaHw8aE"
      },
      "outputs": [],
      "source": [
        "# # Split text\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
        "# docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Review the loaded document\n",
        "# print(f\"{docs[10].metadata}\\n\")\n",
        "# print(docs[10].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYSGzYvRyGNb"
      },
      "outputs": [],
      "source": [
        "vector_db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_db.save_local(\"data/CIA_faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXRITtPxFpy"
      },
      "source": [
        "Load vector from saved index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iiHxzE_AohNs"
      },
      "outputs": [],
      "source": [
        "vector_db = FAISS.load_local(\"data/CIA_faiss_index\", embeddings, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSS-OU-IycZh"
      },
      "source": [
        "Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3Q__zz6VybMq"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZw8ojnZomfR"
      },
      "source": [
        "Chat setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y9Tbv_08on9W"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short, succinct and informative, so that the couterpart can learn from you.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# chat_history = []\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "doc_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, prompt\n",
        ")\n",
        "\n",
        "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# rag_bot = create_retrieval_chain(\n",
        "#     doc_retriever, doc_chain\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDclu2qqZh8"
      },
      "source": [
        "Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG89RzK5qcBS",
        "outputId": "1cf6882d-7796-4b6a-f1ff-ac66ce430ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Assistant: 4.4% (2017 est.)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Response: Germany celebrates several festivals throughout the year, but the most widely recognized include:\n",
            "\n",
            "1. Oktoberfest: Held in Munich, this famous beer festival runs from late September to the first weekend in October.\n",
            "2. Christmas (Christkindlesmarkt): A festive market and fair that takes place in many German cities, with the best-known Christmas markets in Nuremberg, Dresden, and Heidelberg.\n",
            "3. Carnival (Fasching): Predominantly celebrated in southern Germany, particularly in Cologne, Mainz, and Munich, Fasching is a time of parades, costumes, and street parties leading up to Lent.\n",
            "4. Easter (Osterspaziergang): Easter is a significant religious festival celebrated with church services, processions, and traditional foods like Ostersuppe (Easter soup).\n",
            "5. Walpurgis Night (Valborg): Celebrated on the last Friday before May Day (April 30), this spring festival is marked by bonfires, dancing, and singing to ward off evil spirits.\n",
            "6. New Year's Eve (Silvester): Similar to New Year's Eve celebrations in other countries, Germans often gather in public squares, clubs, and private parties to watch the fireworks and countdown to the New Year.\n",
            "\n",
            "\n",
            "Human: What is the capital of germany?\n",
            "Response:\n",
            "Assistant: Berlin\n",
            "\n",
            "\n",
            "Human: What is the official language of germany?\n",
            "Response: The official language of Germany is German.\n",
            "\n",
            "\n",
            "Human: What is the currency used in germany?\n",
            "Response: The currency used in Germany is the Euro (€).\n",
            "\n",
            "\n",
            "Human: Are there any major rivers in germany?\n",
            "Response: Yes, Germany has several major rivers. The Rhine River is one of the most significant, flowing through several German states and into the Netherlands and Belgium. Other important rivers include the Danube, which flows entirely within Germany, and the Elbe, which also runs through Germany and into the Czech Republic.\n",
            "\n",
            "\n",
            "Human: What is the population of germany?\n",
            "Response: According to estimates, the population of Germany is approximately 83 million people as of 2021. However, please note that population figures are subject to change over time.\n",
            "\n",
            "\n",
            "\n",
            "Response:\n",
            "Assistant: \n",
            "\n",
            "\n",
            "Human: What is the GDP composition by sector of origin in Germany?\n",
            "Response: \n",
            "Assistant: Agriculture—products:\n",
            "\trubber\tand\tsimilar\tproducts,\tpalm\toil,\tpoultry,\tbeef,\n",
            "forest\tproducts,\tshrimp,\tcoffee,\tmedicinal\therbs,\tessential\toil,\tfish\tand\tits\n",
            "similar\tproducts,\tand\tspices\n",
            "Industries:\n",
            "\t petroleum\t and\t natural\t gas,\t textiles,\t automotive,\t electrical\n",
            "appliances,\t apparel,\t footwear,\t mining,\t cement,\t medical\t instruments\t and\n",
            "appliances,\thandicrafts,\tchemical\tfertilizers,\tplywood,\trubber,\tprocessed\tfood,\n",
            "jewelry,\tand\ttourism\n",
            "Industrial\tproduction\tgrowth\trate:\n",
            "\t3.8%\t(2017\test.)\n",
            "country\tcomparison\tto\tthe\tworld:\n",
            "\t76\n",
            "Labor\tforce:\n",
            "\t126.1\tmillion\t(2017\test.)\n",
            "country\tcomparison\tto\tthe\tworld:\n",
            "\t5\n",
            "Labor\tforce—by\toccupation:\n",
            "\t\n",
            "agriculture:\n",
            "\t32%\n",
            "industry:\n",
            "\t21%\n",
            "services:\n",
            "\t47%\t(2016\test.)\n",
            "Unemployment\trate:\n",
            "\t5.4%\t(2017\test.)\n",
            "5.6%\t(2016\test.)\n",
            "country\tcomparison\tto\tthe\tworld:\n",
            "\t72\n",
            "Population\tbelow\tpoverty\tline:\n",
            "\t10.9%\t(2016\test.)\n",
            "Household\tincome\tor\tconsumption\tby\tpercentage\tshare:\n",
            "\t\n",
            "lowest\t10%:\n",
            "3.4%\n",
            "highest\t10%:\n",
            "\t28.2%\t(2010)\n",
            "Distribution\tof\tfamily\tincome—Gini\tindex:\n",
            "\t36.8\t(\n",
            "Ending the conversation. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "chain = create_retrieval_chain(\n",
        "    doc_retriever, doc_chain\n",
        ")\n",
        "history = []\n",
        "# Start the conversation loop\n",
        "while True:\n",
        "  user_input = input(\"\\nYou: \")\n",
        "\n",
        "  # Check for exit condition\n",
        "  if user_input.lower() == 'end':\n",
        "      print(\"Ending the conversation. Goodbye!\")\n",
        "      break\n",
        "\n",
        "  # Get the response from the conversation chain\n",
        "  response = chain.invoke({\"input\":user_input, \"chat_history\": history, \"context\": retriever})\n",
        "  history.extend([{\"role\": \"human\", \"content\": response[\"input\"]},{\"role\": \"assistant\", \"content\":response[\"answer\"]}])\n",
        "  # Print the chatbot's response\n",
        "  print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cED7Wah0l8l"
      },
      "source": [
        "Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRAfwcEV1IPi"
      },
      "source": [
        "app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFsof3G1NHL",
        "outputId": "b1f38341-704f-4695-da4e-bb7db31cb9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VVq050eE02v9",
        "outputId": "212957fe-6528-4caf-80cf-db8abb210f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data/streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data/streamlit_app.py\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = 'microsoft/Phi-3.5-mini-instruct' # 'mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-l6-v2'\n",
        "embeddings_folder = '/content/'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local('data/CIA_faiss_index', embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "Answer the question based only on the following context and previous conversation.\n",
        "Keep your answers short, succinct and informative, so that the couterpart can learn from you.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', template),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human', '{input}'),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title('CIA World Factbook 2018-2019')\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input('Curious minds wanted!'):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message('human').markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({'role': 'human', 'content': prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner('Asking CIA...'):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({'input': prompt, 'chat_history': st.session_state.messages, 'context': retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer['answer']\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message('assistant'):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({'role': 'assistant', 'content':  response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSFfQQfA0ndC"
      },
      "outputs": [],
      "source": [
        "def tunnel_prep():\n",
        "    for f in ('cloudflared-linux-amd64', 'logs.txt', 'nohup.out'):\n",
        "        try:\n",
        "            os.remove(f'/content/{f}')\n",
        "            print(f\"Deleted {f}\")\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "    !nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "    url = \"\"\n",
        "    while not url:\n",
        "        time.sleep(1)\n",
        "        result = !grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1\n",
        "        if result:\n",
        "            url = result[0]\n",
        "    return display(HTML(f'Your tunnel URL <a href=\"{url}\" target=\"_blank\">{url}</a>'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "0_chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
